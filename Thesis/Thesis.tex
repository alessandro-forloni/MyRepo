\documentclass[12pt]{article} %[a4paper]{article}

\usepackage{caption}
\usepackage{subcaption}
% accetta caratteri complicati tipo vocali accentate
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


%collegamenti ipertestuali
\usepackage{hyperref}

%tabelle
\usepackage{booktabs}
\usepackage{caption}

%grafici
\usepackage{graphicx}
\usepackage{subcaption}

%matematica
\usepackage{mathtools}
\usepackage{amsmath}

%stile di pagine
\pagestyle{plain} %headings?

%Notes
\usepackage{fancyhdr}
\usepackage{wrapfig}% layout
\usepackage{graphicx}				%import images
\usepackage[export]{adjustbox}
\graphicspath{{Images/}}									% permet d'aller chercher des fichiers (utilise graphicx)
\usepackage[colorinlistoftodos]{todonotes}						% pour faire des notes	
\usepackage[toc,page]{appendix}
\usepackage{caption}
\usepackage{subcaption}


\usepackage{enumerate}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{a4paper}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{color}
\usepackage[tt]{titlepic}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{lastpage}
\usepackage[labelformat=simple]{subcaption}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{tasks}
\usepackage{float}
%\pagenumbering{gobble}
\numberwithin{equation}{subsection}

\begin{document}

\newcommand*\xbar[1]{%
  \hbox{%
    \vbox{%
      \hrule height 0.5pt % The actual bar
      \kern0.5ex%         % Distance between bar and symbol
      \hbox{%
        \kern-0.1em%      % Shortening on the left side
        \ensuremath{#1}%
        \kern-0.5em%      % Shortening on the right side
      }%
    }%
  }%
} 

%\interfootnotelinepenalty=10000
\begin{titlepage}
	\vspace{20mm}
	\begin{center}
		\includegraphics[width=5cm]{Figures/EPFL_LOG.pdf}\\
		\vspace{2cm}
		%{\large{\bf }}\\
		\vspace{5mm}
		{\LARGE{\bf A Portfolio Allocation Framework \\ \vspace{5pt} for Algorithmic Trading Strategies}}\\
		\vspace{3mm}
		{\large{\bf Master Project}}\\
		\vspace{19mm} {\large{\bf In cooperation with NAFORA SA}}
	\end{center}
	\vspace{20mm}
	\par
	\noindent
	%\begin{center}

			{\large{\bf Realized by: \\
					ALESSANDRO DANIELE FORLONI\\ \vspace{10pt}
					
					\bf Supervised by: \\
					YILIN HU and SEMYON MALAMUD\\ \vspace{10pt}
				}}
	%\end{center}
	
	\vspace{5mm}
	\begin{center}
		{\large{\bf Academic Year\\
				2017/2018 }}
	\end{center}
\end{titlepage}

\newpage
\tableofcontents

\newpage

\section*{Acnkowledgments}
%\addcontentsline{toc}{section}{Acnowledgments}

\section{Introduction}\label{intro}
%\addcontentsline{toc}{section}{Introduction}

In this thesis we address the challenge of portfolio allocation applied to a set of trading algorithms. The aim here is to allocate risk to many algorithmic trading strategies on a weekly basis. The problem is to be solved in two seprate parts: firstly address at any moment in time which strategies out of the many available to put into production and at last assign proper risk weights to these strategies. We will see that both the steps are to be addressed with care as none of these problem if solved alone can achieve satisfactory results. All the results will be compared with a proper benchmark that mimics the current non-systematic allocation strategy. If the procedure is will be implemented it will make the whole investing process completely systematic. The weekly allocation period is chosen as it fits at best the charachteristics of the market of interest and it avoids incurring in excessive transaction costs arousing from daily rebalancing of the portfolio that would erase any improvement given by the selection methodology.\\
The challenges that have been faced include the abundance of strategies and the well known issue that alpha in algorithmic trading strategies is not everlasting. There is a point in time at which any strategy will stop working and will necessarily be switched off, on the other hand, as a reaction to market changes, some strategies that in the past performed poorly might become alpha generators. Achieving optimal timing in putting into production and swithcing off the strategies represents a challenge but also an opportunity to substantially increase trading performance.This task is hard to perform in other ways than algorithmic selection because often what is selected might not look intuitive to trade at first sight. Inter-market relationships change through time. \todo{CONTROLLA CHE NON SIA UNA CAZZATA} For example, if one is trading on the well known relationship between Gold and US Government Bond (which is expected to be stedily meaningful and potentially a good source of alpha), it might be that due to some specific event this correlation breaks down, changing all the underlying market-dynamics and making the algorithms unprofitable. Moreover, in some cases, a certain strategy might perform really well for years until somebody in the market strats exploiting it systematically and at high frequency bringing liquidity and margin for trading out of the scope of hedge funds. In such cases detecting a switching point in the performance of strategies is of crucial importance.\\


%IF WE NEED TO CITE SOMETHING USE: \cite{example}.





\subsection{Literature Review}
%\addcontentsline{toc}{subsection}{Literature Review}

The problem of portfolio optimization is one of the oldest and most discussed topics in finance. The traditional theory that has been just discussed has been developed at first by Harry Markowitz who won the Nobel Prize for his article \textit{Portfolio Selection} in 1952 \cite{Markowitz}. The Modern Portfolio Theory that was developed in those years is actually a milestone of finance, and still today it is regarded as the baseline for portfolio managers. Unfortunately, Markowitz allocation procedures have some drawbacks, mainly relative to the numerical instability of the estimation of asset returns. Some scholars improved through the years the models trying to add stability. In 1992 a huge step forward was made by Fisher Black and Robert Litterman with their article \textit{Global Portfolio Optimization} \cite{black_litterman} that merged the CAPM equilibrium theory with the mean-variance optimization proposed by Markowitz. Their idea was that incorporating meaningful information coming from CAPM and personal views of portfolio managers would solve the issues of unreasonableness of quantitative portfolios. More recently, an additional step was made by Olivier Ledoit and Michael Wolf whose contribution was to improve the global performance of covariance matrices by introducing the idea of \textit{shrinkage}. This method allows to have more stable covariance matrices and therefore making the Markowitz framework more stable and applicable.  \\
Many experts from different fields have worked to enrich the knowledge in this specific area. A well-known example is that of Professor Cover, whose work is recognized as one of the finest attempts tu use signal theory in portfolio allocation. With his article \textit{Universal Portfolios} \cite{universal_portfolios} he builds a portfolio, that in terms of performance, asympthotically beats the best stock over a given set of stocks. The interesting part of cover's work is that he attempts to solve the portfolio optimization puzzle in a non-parametric way, using robust results, this unfortunately comes at the expense of not universal applicability.\\
Recently, with the advent of Machine Learning, many experts started applying powerful algorithms to portfolio selection with interesting results. Any kind of use has been made, from forecasting returns to allocate risk to cluster assets to create well-diversified portfolios. The increase of computational power has allowed to test on large scale portfolios complex algorithms like genetic learning models or neural networks. These have been used in many ways, for example some researchers in the US have trained a one-layer recurrent neural network to dynamically optimize a portfolio \cite{NN_1}. Others tried to forecast asset returns with a specific Hopfield Neural network to input into a traditional mean-variance Markowitz style optimization \cite{NN_2}.\\
Despite their out-of-sample results are not outstanding, these pieces of work set with others a new path for portfolio optimization that extracts the most information out of the available data. We will follow this path trying to optimize our portfolio using the most information as possible. In particular we will follow the approach of some researches in te area of genetic algorithms \cite{genetic_1} and that of Marcos Lopez de Prado, that aims at building well-diversified portfolios with unsupervised clustering methods \cite{HRP}.\\
More than 50 years after the first attempt to address the issue of portfolio selection, Markowitz models are still regarded as the baseline model around which all the theory is built. These models still have relevant real-world issues such as ignorance of transaction costs and high instability of the portfolio, but are really intuitive and representative of the dynamics of a rational investor.  


\subsection{Our Approach}
%\addcontentsline{toc}{subsection}{Our Approach}

The "schedule" we set at the beginning is to find firstly a satisfactory method to switch strategies on and off and then move to the part of weight allocation. We will consider the first step to be completed once the resulting selection allows for efficient trading of around a hundredth of strategies with at least half of the trading days with positive pnl.\\
\todo{QUI E' TUTTO DA VERIFICARE ED AGGIORNARE}
To achieve this step a simple and robust feature-based approach has been used. We decided not to try to use any hard-core machine learning type approach to reduce the risk of overfitting and to fit the specificity of the problem. In fact, the abundance of strategies and the lack of a long samples would have made traning a machine learning method cumbersome and time-consuming.\\
Before getting into this part, relevant features are needed to give some predictive insight to our models. To this end we built several different features and evaluated their predictive power through a \textit{Random Forest Tree} (details of this model will be provided later).\\
For what concerns assigning the weights we developed two different approaches, one of which is more computationally oriented and the other is more diversification-driven. The first approach is to train a genetic portfolio allocator that will select the best portfolio in-sample and then apply iy out of sample. The second approach is based on clustering, and aims at reducing as much as possible the realized variance of the portfolio.\\

Aggiungi altri dettagli: cioè quello che aggiungiamo rispetto alla teoria classica:
\begin{itemize}
	\item cross-validation
	\item robust feature importance
	\item rigorous separation of in-sample, out of sample and production set
	\item unsupervised methods with little assumptions
\end{itemize}


\subsection{The data}
%\addcontentsline{toc}{subsection}{The Data}

As mentioned before the dataset at hand consists of 13000 simulated strategies, based on mean-reversion. All these strategies are trading one futures against another one looking for relative mispricings. This number of strategies comes out of a simulation of all possible trading pairs among roughly 150 futures traded worldwide. The huge diversity among these strategies makes it hard to find a unique model to allocate risk among all of them. Diversification has to be applied not only to asset classes, but also taking into account type of algorithms, trading latency and underlying country or region of exposure.\\
The data spans through almost six years, from January 2012 to August 2017. To perform the studies, the final year has been dropped to be used as a final validation set (also referred to as production set). The first remaining chunk has been divided into train and test set.\\
We also decided to remove any strategy involved in swiss franc trading, to avoid our results to be biased by the famous drop that happened on the 15th of January 2015. We don't want to penalize or advantage any strategy that happened to be trading the swiss franc in either long or short side in that day because we believe that was a statistically unpredictable event. There is no guarantee that such an event could be forecasted only with information coming from strategy performance.\\

\subsection{Some Descriptive Statistics}
%\addcontentsline{toc}{subsubsection}{Some Descriptive Statistics}

Here we want to give a taste of what our data looks like. We first run a code that computes sample statistics for all the strategies. The results are exposed in Table 1. 

\begin{table}
	\centering
	\begin{tabular}{c|c|c|c|c|c}
		Statistic   & Mean & Median & Min & Max & IQR \\\hline
		Mean Return & -0.0002 & -0.0002 & -0.0032 & 0.0024 & 0.0002 \\ 
		Skew        & -1.9393 & -1.698 & -34.2374 & 21.119 & 2.5754 \\ 
		Kurtosis    & 56.9226 & 25.5496 & -5.4507 & 1202.95 & 37.851 \\ 
		Sharpe Ratio& -1.3653 & -1.1725 & -20.0072 & 15.547 & 1.4080 \\ 
		Sortino     &-1.3514 & -1.1808 & -32.9069 & 40.322 & 1.3692 \\ 
	\end{tabular}
	\caption{\label{tab:widgets} Global strategy statistics.}
\end{table}

We can see how on average the mean return per strategy is negative. The sharpe ratio of course follows this pattern as well. On the other hand we notice how some sharpes are very high (e have peaks at around 15 on the wole history!). Here an important remark must be made, many strategies with good performance seem really appealing, but for several reasons might not be tradable due to liquidity issues, regulation or asynchronization of quotes data.\\
Back to our statistical analysis, we notice how the skew and kurtosis reach extreme values, signaling that the returns of these strategies might not be normally distributed. To this end we conducted a Shapiro-Wilks normality test for each strategy, where the null Hypothesis of normality is challenged (for details on this procedure refer to the Appendix). The results are the following:

We can observe that for the majority of the cases the normality hypothesis is rejected. Some strategies survive the test, but a deeper analysis supports the idea that this is caused by a lack of data for these strategies. 

\todo{AGGIUNGI STYLIZED FACTS PER CONFRONTARE CON RITORNI SOLITI}
\todo{AGGIUNGIGRAFICO AUTOCORRELATION STRATEGIE}

\section{Classic Portfolio Theory}

\subsection{Risk and Return}

\input{Portfolio_Theory/Risk_n_Return}

\subsection{A simple two-asset example}

\input{Portfolio_Theory/two_assets}

\subsection{Extending to N assets}

\input{Portfolio_Theory/N_assets}


\subsection{Why aren't Markowitz portfolios optimal?}

\input{Portfolio_Theory/non_optimality}


\subsection{The risk-free asset}

\input{Portfolio_Theory/risk_free}


\subsection{Performance Metrics}

\input{Portfolio_Theory/performance_metrics}




\section{Part 1: Strategy Selection}
%\addcontentsline{toc}{section}{Part 1: Strategy Selection}

\subsection{Problem Statement}
%\addcontentsline{toc}{subsection}{Problem Statement}

We give here an additional re-statement of the problem we try to tackle here. On each monday we have to allocate risk on each of the given strategies by choosing which ones to put into production for the following week. In an ideal world we would switch on all the strategies that will perform well during the following week and vice-versa with the bad ones. Unfortunately this is quite an impossible task, and we just seek a "statistical edge" that allows us to profit from appropriate selection of strategies on the long run.\\
For this first part we focus only on activating or de-activating the algorithms, we don't care about the risk weight to give to strategies (the output will be a $\{1,0\}$ signal).

\subsection{Building the Features}
%\addcontentsline{toc}{subsection}{Building the Features}

\input{Part_1/building_features}


\subsection{Relevant Features}
%\addcontentsline{toc}{subsection}{Relevant Features}	

\input{Part_1/relevant_features}


\subsection{An Additional Test}

\input{Part_1/t_test}

\subsection{Switching Model}
%\addcontentsline{toc}{subsection}{Switching Model}

Here we dwelve into the challenge of finding an optimal subset of all the strategies to put in production for a given week. The model here should just tell us whether a strategy is good or bad for the coming week.\\
As opposed to a traditional machine learning model, we want something more simple, interpretable and faster. Following the results of our random forest tree classifier we decided to base our models on robust thresholds or ranking of strategies. The reasons for dropping cutting-edge machine learning methods in this context are multiple: first we don't have many samples per strategy to be able to train machine learning models. If we had more data, a Neural Network could for example learn the complex relationships between feature behaviour and future performance of the strategy, but since we have little amounts of data per strategy this could be hardly achievable. It is quite clear that in this context we are aiming at training individually each strategy, as trying to fit many different strategies in one model is definitely not optimal.\\
We have tested several different models and evaluated them based on the performance they provided. This performance is not only a function of raw PnL, but is at first risk-adjusted and secondarily it takes into account how volatile the allocation is. It is well established that some interesting theoretical models just don't work in the real world because of transaction-costs and implementation issues. We want to avoid falling into this problem, finding something that works really well in-sample but then requires to completely reshuffle the portfolio each week killing any kind of intrinsic alpha.
Each model will require to fit various parameters, therefore we will perform in-sample gridsearches looking for pseudo-optimal parameters trying to avoid overfitting. That means we will look at the results with common sense, if we will find very surprising numbers we will dwelve in the nature of these parameters, and eventually we might not validate the results we see.\\


\subsection{First Tests}
%\addcontentsline{toc}{subsection}{First Tests}

We run different tests (in our in-sample period) to see which meaningful combination of features could come up with a proper switching model. At first, we tried to base our models only on one feature and it turned out that using only one feature was not enough as the data is really diverse and many strategies have very poor performance. This fact forced us to somehow use a second feature to act as a filter that would wipe out of the full set of strategies a huge majority that had not been performing well in the past. We directed our endeavours towards finding this meaningful filter of strategies. What this filter has to do, is to look at the past performance of any single strategy and set a threshold below which even if the current performance is good this strategy would not be switched on. The reason for this is that many strategies have some very short period where they work well due to specific market conditions that don't last for long. We luckily have a huge wealth of strategies and we can afford being strict in selecting strategies giving more strength to our method. After some tests and discussions we decided that a good filter is given by a long-window rolling-sharpe ratio. This feature looks at the performance of a strategy and computes with a certain rolling window the Sharpe. Then we look at the resulting time series and we require the sharpe to be sufficiently good over a certain window in history. In other words, every Monday the historical x-days sharpe ratio for each strategy is computed and if we are able to find  a period of x-days when a strategy performed sufficiently well in terms of sharpe we believe this is a strategy that can be switched on and off in the future. Evidence will show that this lookback window should be quite long, proving that the strategy has been working for quite a long time in the past (See following parts). Once this filter is applied, the remaining features are switched according to information coming from another feature that we will explore in the following part.\\
Armed with this tool we started building many different models with all the features we had at hand. Results showed that the Random Forest tree was almost always right in estimating the predictive power of features. In fact, we noticed that portfolios based on features like Exponential Moving Averages and Hit Ratio (features with low predictive power) didn't perform as well as portfolios based on sharpe-ratio. We can see some results below:

\todo{METTI GRIDSEARCH CAGOSA}

Moreover, an interesting aspect to analyize is the set of strategies switched by these methods, it seems that they tend to select very similar strategies as the ones switched by the Sharpe-Ratio based portfolios, but with much worse market timing.\\

\subsection{Method 1: Pure Sharpe}
\input{Switch_Models/Method_1}



\subsection{Method 2: Ranking Based}
%\addcontentsline{toc}{subsection}{Method 2: Ranking Based}

\input{Switch_Models/Method_2}


\subsection{Method 3: Drawdown Based}

\input{Switch_Models/Method_3}

\section{Part 2: Risk Allocation}

Once we have a robust and trustable switching method, we can move our scope towards risk minimization, or in more precise terms, sharpe ratio maximization. We will build on top of the selected portfolio two different weights systems that will be benchmarked against a simple equally weighted portfolio and a Markowitz-like minimum variance portfolio (see appendix for building details). As it was for the switching problem, our aim is still to find the best out-of-sample portfolio for the following week (setting the new weights on monday) given information up to the previous friday. \\

\subsection{Model 1 - A Genetic Learner}

\input{Genetic_Algo/Genetic_Intro}


\subsubsection{Implementation}

\input{Genetic_Algo/Implementation}

\subsubsection{Fitness Function}

\input{Genetic_Algo/Fitness_Function}

\subsubsection{Converging towards a global optima}

\input{Genetic_Algo/Convergence}

\subsection{Model 2 - An enhanced risk-parity}

\input{HRP/Intro}

\subsubsection{Clustering the strategies with affinity propagation}

\subsubsection{Allocating risk}



\section*{Results}

\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}


\subsection*{Shapiro-Wilks normality test}
\addcontentsline{toc}{subsection}{Shapiro-Wilks normality test}

As suggested by the name, the Shapiro-Wilks test checks if a sample is drawn from a normal distribution. More precisely, given a sample it tests $H_0$ (normality) versus the alternative hypothesis of non-normality. This test is ideal for our case as it doesn't require too much data to come to a conclusion. The test is non parametric and starts with sorting the data. Once the data is sorted, the test statistic can be computed:

$$
\displaystyle W = \frac{\left(\sum\limits_{i=1}^N a_{i}x_{(i)}\right)^2}{\sum\limits_{i=1}^N(x_i-\bar{x})^2}
$$

Each element has its specific meaning:
\begin{itemize}
	\item $\bar{x}$ is the sample mean of the data.
	\item $x_{(i)}$ is the $i$-th order statistic. 
	\item $a_i$ are tabulated coefficients coming out of the distribution of order statistics of a normal standard distribution.
\end{itemize}

The larger the statistic the more "normal" the data. This comes from the idea that the test wants to measure the similatity of the ordered statistics to those of a standard normal distribution. The W statistic somehow measures the closedness of these two entities.

\subsection*{Minimum Variance Portfolio}
\addcontentsline{toc}{subsection}{Minimum Variance Portfolio}

Here we build the foundations of the Minimum Variance portfolio used as a benchmark to measure the relative performance of our weight assignment methods.\\
Firstly, we set the problem in rigorous terms: given a set of N tradable instruments (in our case trading strategies) we want to find the optimal trading vector $\mathbf{w} = (\mathbf{w_1} \dots \mathbf{w_N})$ that represents the composition of our portfolio. This composition will optimally be the one that minimizes the in-sample variance of the portfolio. The latter is measured as:

$$
\sigma^2_\pi = \frac{1}{2} \mathbf{w}^T\mathbf{\Sigma} \mathbf{w}
$$

This optimization problem is usually solved under the constraint that the sum of the weights should be equal to one. We will solve the problem and then impose that the weights are also positive (it wouldn't make sense to trade strtegies with negative weights).\\
The lagrangean to solve to minimize the variance is the following:

$$
\mathbf{L} = \frac{1}{2} \mathbf{w}^T\mathbf{\Sigma} \mathbf{w} - \lambda\left(\mathbf{1}^T\mathbf{w} - 1\right)
$$

Where $\mathbf{1}$ is a vector made up of ones.\\
We compute the first order conditions:

$$
\frac{\partial \mathbf{L}}{\partial \mathbf{w}} = \mathbf{\Sigma} \mathbf{w} - \lambda\mathbf{1} = 0 \qquad  \frac{\partial \mathbf{L}}{\partial \lambda} = \mathbf{1}^T\mathbf{w} - 1 = 0
$$

From the first F.O.C. we immediately find:

$$
\mathbf{w} = \lambda \mathbf{\Sigma}^{-1} \mathbf{1}
$$

We plug this result into the other F.O.C.:

$$
\lambda \mathbf{1}^T \mathbf{\Sigma}^{-1} \mathbf{1} - 1 = 0 \Longrightarrow \lambda = \frac{1}{\mathbf{1}^T \mathbf{\Sigma}^{-1} \mathbf{1}}
$$

Therefore getting a nice analytical closed-form solution for our minimum variance portfolio:

$$
\mathbf{w} = \frac{\mathbf{\Sigma}^{-1} \mathbf{1}}{\mathbf{1}^T \mathbf{\Sigma}^{-1} \mathbf{1}}
$$

The beauty of this formula comes with some drawbacks:
\begin{itemize}
	\item $\mathbf{\Sigma}$ is often not precisely estimated due to the huge number of strategies and the little amount of samples to use to measure standard deviations and correlations. Moreover this matrix is not to invert leading to numerical errors. To partially address these issues we use a \textit{LedoitWolf} covariance matrix whose construction is explained in the next chapter.
	\item This approach completely ignores transaction costs, leading to a fastly changing and unstable portfolio composition
	\item The model works making a basic assumption: in-sample correlations and variances will hold out-of-sample with very simila values. Unfortunately this is rarely the case in the real world, making this portfolio sub-optimal in terms of variance.
\end{itemize}

\subsection*{Ledoit Wolf Covariance Matrix}
\addcontentsline{toc}{subsection}{Ledoit Wolf Covariance Matrix}

\subsection*{Random Forest Tree}
\addcontentsline{toc}{subsection}{Random Forest Tree}

As outlined before, the decision trees are an all-purpose machine learning algorithm able to be trained on extremely non-linear phenomena. The beauty of these algorithm lies in the simplicity of the underlying learning process, the data is split in "sectors" in a way that the highest "purity" is achieved. The Random forest algorithm adds robustness to this process. Let's first explore in detail the training process for a simple decision tree.\\

\begin{itemize}
	\item  Given an m-dimensional set of data with an output feature (we are in the case of supervised learning) examine all the possible splits on one feature.
	\item Evaluate each split based on the purity of the splitted areas. This is done trough the \textit{Gini} impurity measure: $I_G = \sum\limits_{i=1}^N p_i(1-p_1)$, where $N$ is the number of labels/classes in the data.\\
	Ths measure indicates how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. This comes clear with the fact that the tree assigns probability to labels.
	\item The purest split gives rise to a new node.
	\item From this split others are generated until the highest purity or the maximum number of splits is achieved.\\
\end{itemize}

As it might emerge from this brief explanation, decision trees tend to overfit the data, as they learn very complex non linear-features. This behaviour is described in the context of the bias-variance trade-off where decision trees stand more in favor of variance rather than bias. Random forest try to overcome this issue by averaging many trees.\\
Once we understood how a general decision tree is trained we can explore in depth the training of a random forest algorithm:

\begin{itemize}
	\item Generate M different trees.
	\item Each tree is trained (as a normal decision tree) on a random subset of the features. This number is usually believed to be a fraction $\sqrt{n}/n$ where $n$ is the number of features.
	\item The results from all the trees are averaged, that means that for each point the final label will be given by the average of all labels given by the different M trees. 
\end{itemize}

This robust procedure is useful to train powerful regressors or classifiers, but might be used as well to measure the forecasting ability of the input features. If a feature has real predictive power, it will be used in many bootstrapped samples to produce splits in the data, therefore being used many times. Computing the number of times each feature is used to produce a split will give a ranking of feature importances.\\

\newpage
%\section*{Appendix}
\subsection*{Figures and Tables}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{Figures/moving_average.eps}
	\caption{Rolling average of the returns for 4 industries and Rf}
	\label{rolling_average}
\end{figure}








\clearpage

\begin {table}[htbp]
\begin{center}
\input{Figures/industry_summary.tex}
\end{center}
\caption {Descriptive statistics for simple returns} \label{industry_summary} 
\end {table}



\newpage
%\clearpage

\bibliographystyle{unsrt} 
\bibliography{Biblio}



\end{document}
